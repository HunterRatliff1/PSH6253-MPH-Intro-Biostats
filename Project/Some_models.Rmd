---
title: "Some models"
author: "Hunter Ratliff"
date: "11/17/2019"
always_allow_html: true
output:
  html_document:
    theme: united
    toc: yes
    toc_float: yes
  pdf_document:
    df_print: kable
    highlight: tango
    toc: yes
  word_document: 
    toc: yes
---

Data files to reporoduce this analysis can be found on [my GitHub](https://github.com/HunterRatliff1/PSH6253-MPH-Intro-Biostats/tree/master/Project/data)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F, warning=F, message=F, collapse = T)
library(pander)
library(scales)
library(ggthemes)

library(sjPlot)
# library(sjlabelled)
# library(sjmisc)


# library(randomForest)
# require(caTools)
# library(caret)
library(leaps)
library(MASS)

# library(survey) # dsgn <- svydesign(ids=~CASEID, strata = ~strata, weights=~wgt, data=data)
# library(Epi)        # Epi::clogistic for conditional logit
# library(survival)   # survival::clogit() works like Epi package, but with weights
# library(rstanarm)   # rstanarm::stan_glm() for Bayesian models
library(kableExtra)
library(broom)
library(tidyverse)

theme_set(theme_sjplot())
```

# Set reference levels

The reference levels can be a little confusing for the regression, so I'll explain it here.

For predictors (i.e. _independent variables_), the reference is exactly what it sound like. So if I set the reference level for `race` to be white, the estimate for coefficients race`XYZ` is in reference to white subjects, as shown below:

```{r, eval=FALSE, echo=T}
BIRTH %>%
  mutate(
    race = relevel(race, "White")
  )

# Coefficients:
#                Estimate Std. Error z value Pr(>|z|)    
# ...              ...        ...       ...   ...
# raceHispanic  3.476e-02     ...       ...   ...
```

For our our _outcomes_ it's the **second** level of the factor that becomes our outcome (or "success"). So if the reference is set to having a normal birthweight, then the odds ratio is representing the odds ratio of **being born low birth weight**.


```{r readData}
# Read in data
BIRTH <- read_rds("data/BIRTH.RDS") %>%
  mutate(
    LBW = factor(ifelse(LBW, "Low", "Normal")),
    Wanted = factor(ifelse(Wanted, "Yes", "No")),
    
    ### --------------- Set default levels ---------------
    ## For OUTCOME variables - see help(binomial)
    # For using factors in binomial: 'success' is interpreted as the factor 
    # NOT having the first level (success=second level)
    LBW   = relevel(LBW,   "Normal"),
    PreMe = relevel(PreMe, "Term"),
    
    ## For PREDICTORS, the first level is the ref
    Wanted = relevel(Wanted, "Yes"),
    race   = relevel(race,   "White")
  )

PN    <- read_rds("data/PN.RDS") %>%
  mutate(
    LBW = factor(ifelse(LBW, "Low", "Normal")),
    Wanted = factor(ifelse(Wanted, "Yes", "No")),
    
    ### --------------- Set default levels ---------------
    ## For OUTCOME variables - see help(binomial)
    # For using factors in binomial: 'success' is interpreted as the factor 
    # NOT having the first level (success=second level)
    LBW   = relevel(LBW,   "Normal"),
    PreMe = relevel(PreMe, "Term"),
    
    ## For PREDICTORS, the first level is the ref
    Wanted = relevel(Wanted, "Yes"),
    race   = relevel(race,   "White")
  )
```


Below are the reference levels for the `BIRTH` dataset

```{r}
BIRTH %>% select_if(is.factor) %>% select(-eduCat) %>% str()
```

Below are the reference levels for the `PN` dataset

```{r}
PN %>% select_if(is.factor) %>% select(-eduCat) %>% str()
```


# Low Birth Weight

First, a refresher on the number of subjects who were low birth weight

```{r}
table(BIRTH$LBW) %>% 
  pander(caption="Counts of subjects with normal and low birth weights")
```

This model predicts being born as **low birth weight** for their gestational age with the formula below

```{r}
mod.LBW <- glm(LBW ~ GA + BMI + age + income + race + YrEdu + Wanted, 
    data=BIRTH, family=binomial(link = "logit"))
pander(mod.LBW)
```

Note that the estimate above is the **Log-odds**. The table below transforms the log-odds into odds ratios

```{r}
# Tidy version of summary
broom::tidy(mod.LBW) %>%
  select(term, estimate, p.value) %>%
  mutate(Odds.Ratio = exp(estimate)) %>%
  select(-estimate) %>%
  cbind(exp(broom::confint_tidy(mod.LBW))) %>%
  mutate_if(is.numeric, round, 4) %>%
  filter(term!="(Intercept)") %>%
  select(term, Odds.Ratio, conf.low, conf.high, p.value) %>%
  # mutate(Odds.Ratio = round(Odds.Ratio, 3)) %>%
  # mutate(conf.low   = round(conf.low, 3)) %>%
  # mutate(conf.high  = round(conf.high, 3)) %>%
  # pander()
  mutate(
    p.value    = cell_spec(p.value, color = ifelse(p.value <0.05, "red", "black"))
    ) %>%
  knitr::kable(digits=4, escape=F) %>%
  kable_styling("striped", full_width = F)
# # Also can do this --> exp(cbind(OR = coef(mod.LBW), confint(mod.LBW))) %>% round(3)    

```

```{r}
glance(mod.LBW) %>% pander()
```


```{r OR_plot1, include=F}
sjPlot::plot_model(mod.LBW, title = "OR of having LBW child",
                   sort.est=T, show.values = T, value.offset = .4)
```

## Type III Anova

```{r ANOVA_1}
# type 3 anova
car::Anova(mod.LBW, type = 3)
```

## Hosmer-Lemeshow & more

```{r diagnostics_1}
# diagnostics
rms::lrm(mod.LBW[["formula"]], 
         data=BIRTH) %>% 
   pander(coefs=F)


# # Global signifigance
# 1-pchisq(
#   mod.LBW[["null.deviance"]]-mod.LBW[["deviance"]],
#   mod.LBW[["df.null"]]-mod.LBW[["df.residual"]]
# )

# Hosmer-Lemeshow Goodness of Fit (GOF) Test
ResourceSelection::hoslem.test(
  ifelse(BIRTH$LBW=="Normal", 0, 1),
  fitted(mod.LBW))
```

**Impression:** Everything looks good here

## Multicolinearity / VIF

Variance inflation factors should be below 5 or 10

```{r VIF_1}
car::vif(mod.LBW)
# cor(model.matrix(mod.LBW)[,-1]) %>% corrplot::corrplot()
```

**Impression:** Looks great, all VIFs are well below 5 (none are even above 2.5)

## Influential values

```{r}
model.data <- broom::augment(mod.LBW) %>% 
  mutate(index = 1:n())
```

```{r}
model.data %>%
  ggplot(aes(index, y=.cooksd)) + 
  geom_linerange(aes(ymin=0, ymax=.cooksd, color=LBW), alpha=0.75) +
  geom_text(aes(x=index, y=.cooksd, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()
```

```{r}
model.data %>%
  ggplot(aes(index, .std.resid)) + 
  geom_point(aes(color = LBW), alpha = .5) +
  geom_text(aes(x=index, y=.std.resid, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()
```

A observations have standardized residuals with an absolute value greater than 3, shown in the table below:

```{r}
# Any standardized residuals above 3?
model.data %>% 
  filter(abs(.std.resid) > 3) %>% 
  select(index, .std.resid, .cooksd, LBW:Wanted, .hat) %>%
  pander(caption="standardized residuals with abs value > 3", digits=3)
```

```{r}
plot(mod.LBW, which = 5)
rm(model.data)
```

## Linearity

Check for a linear relationship between continuous predictors and logit of outcome

```{r linearity_1}
probabilities <- predict(mod.LBW, type = "response")
BIRTH %>%
  select(-CASEID) %>%
  dplyr::select_if(is.numeric)  %>%
  mutate(index = 1:n()) %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit, -index) %>%
  filter(index!=3713) %>% # remove one datapoint 

  ggplot(aes(logit, predictor.value))+
  geom_jitter(size = 0.5, alpha = 0.25) +
  geom_smooth(method = "glm", color="green") + 
  geom_smooth(method = "loess", color="red") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y") +
  labs(
    x = "Logit of predicted values",
    caption="Green line: generalised linear smooth\nRed line: Loess local smooth")
```

**Impression:** `GA` shows a very pretty linear relationship, but all the other relationships don't really look linear at all. This is certainly a problem for this regression, but is less of an issue for the other models.

_Note: one observation has been removed, with a gestational age of 44 weeks_


## Prediction

Using adjusted rates (assuming other variables are at their means)

```{r prediction1a}
### Predictions of propabilities ###
df <- tidyr::expand(BIRTH, Wanted, race) %>%
  mutate(
    GA = mean(BIRTH$GA),
    BMI = mean(BIRTH$BMI),
    age = mean(BIRTH$age),
    # income = mean(BIRTH$income),
    YrEdu  = mean(BIRTH$YrEdu)
  ) %>%
  expand_grid(
    income=seq(0, 500, by=50)
  ) 

# predict(object = mod.LBW, newdata = df, type="response", se.fit = T)
df$fit <- predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$fit
df$ul  <- df$fit + predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$se.fit
df$ll  <- df$fit - predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$se.fit

df %>% ggplot(aes(x=income, y=fit, group=race)) +
  geom_ribbon(aes(ymax=ul, ymin=ll), alpha=0.1) +
  geom_line(aes(color=race)) +
  facet_grid(.~Wanted) +
  scale_color_colorblind() +
  theme_bw() +
  labs(x="Income as % federal poverty line", y="Pr(Having LBW baby)",
       title="Predicted Pr(LBW), by income & race",
       subtitle = "Split by Wantedness (Yes/No)")
```

This prediction assumes Wanted=="Yes

```{r prediction1b}
### Predictions of propabilities ###
df <- tidyr::expand(BIRTH, race) %>%
  mutate(
    # GA = mean(BIRTH$GA),
    BMI = mean(BIRTH$BMI),
    age = mean(BIRTH$age),
    income = mean(BIRTH$income),
    Wanted = as.factor("Yes"),
    YrEdu  = mean(BIRTH$YrEdu)
  ) %>%
  expand_grid(
    GA=seq(20, 45, by=1)
  ) 

# predict(object = mod.LBW, newdata = df, type="response", se.fit = T)
df$fit <- predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$fit
df$ul  <- df$fit + predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$se.fit
df$ll  <- df$fit - predict(object = mod.LBW, newdata = df, type="response", se.fit = T)$se.fit

df %>% ggplot(aes(x=GA, y=fit, group=race)) +
  geom_ribbon(aes(ymax=ul, ymin=ll), alpha=0.1) +
  geom_line(aes(color=race)) +
  # facet_grid(.~Wanted) +
  scale_color_colorblind() +
  theme_bw() +
  labs(x="Gestational age (weeks)", y="Pr(Having LBW baby)",
       title="Predicted Pr(LBW), by gestational age & race")
```

## Cross-Validation

> Ignore this section for grading purposes

Confusion matrix using test & training data

```{r}
# Make two datasets
train <- BIRTH %>% group_by(LBW) %>% sample_frac(.8, replace=T)
test <- anti_join(BIRTH, train)

# Make model
mod <- glm(mod.LBW[["formula"]], data=train, family=binomial(link = "logit"))
predicted <- predict(mod, test, type="response")
```

First, create a training model using 4/5ths of the original dataset. Below are the odds ratios of the original and training model

```{r}
tibble(
  coefficient = names(coef(mod)),
  `Original OR` = exp(coef(mod.LBW)),
  `Training OR` = exp(coef(mod))
) %>%
  mutate_if(is.numeric, function(x) round(x, 2))
```

Next find the misclassifiaction error rate and plot the ROC curve

```{r}
library(InformationValue)
# find optimal cutoff
cutoff <- optimalCutoff(ifelse(test$LBW=="Low", 1, 0), predicted)

# find error rate
misClassError(ifelse(test$LBW=="Low", 1, 0), predicted,
              threshold = cutoff)

# plot ROC
plotROC(ifelse(test$LBW=="Low", 1, 0), predicted)

# Concordance(ifelse(test$LBW=="Low", 1, 0), predicted)
```

Finally, the confusion matrix using test & training data

```{r}
# Confusion matrix
caret::confusionMatrix(positive="Low",
  as.factor(ifelse(predicted>cutoff, "Low", "Normal")),
  reference=test$LBW)

# clean up 
rm(train, test, mod, predicted, cutoff)
```



# Premature birth

Again, a refresher on the number of subjects who were born before 37 weeks

```{r}
table(BIRTH$PreMe) %>% 
  pander(caption="Counts of subjects born at term and prematurely")
```

This model predicts **being born prematurely** with the formula below

```{r}
mod.PreMe <- glm(PreMe ~  BMI + age + income + race + YrEdu + Wanted, 
    data=BIRTH, family=binomial(link = "logit"))
pander(mod.PreMe)
```

Again, table with the transformed odds ratios

```{r}
# Tidy version of summary
broom::tidy(mod.PreMe) %>%
  select(term, estimate, p.value) %>%
  mutate(Odds.Ratio = exp(estimate)) %>%
  select(-estimate) %>%
  cbind(exp(broom::confint_tidy(mod.PreMe))) %>%
  mutate_if(is.numeric, round, 4) %>%
  filter(term!="(Intercept)") %>%
  select(term, Odds.Ratio, conf.low, conf.high, p.value) %>%
  # mutate(Odds.Ratio = round(Odds.Ratio, 3)) %>%
  # mutate(conf.low   = round(conf.low, 3)) %>%
  # mutate(conf.high  = round(conf.high, 3)) %>%
  # pander()
  mutate(
    p.value    = cell_spec(p.value, color = ifelse(p.value <0.05, "red", "black"))
    ) %>%
  knitr::kable(digits=4, escape=F) %>%
  kable_styling("striped", full_width = F)
```


```{r}
glance(mod.PreMe) %>% pander()
```

```{r OR_plot2, include=F}
sjPlot::plot_model(mod.PreMe, title = "OR of having birth prematurely",
                   sort.est=T, show.values = T, value.offset = .4)
```

## Type III Anova

```{r ANOVA_2}
# type 3 anova
car::Anova(mod.PreMe, type = 3)
```

## Hosmer-Lemeshow & more

```{r diagnostics_2}
# diagnostics
rms::lrm(mod.PreMe[["formula"]], 
         data=BIRTH) %>% 
   pander(coefs=F)

# Hosmer-Lemeshow Goodness of Fit (GOF) Test
ResourceSelection::hoslem.test(
  ifelse(BIRTH$PreMe=="Term", 0, 1),
  fitted(mod.PreMe))
```

**Impression:** Everything looks good here

## Multicolinearity / VIF

Variance inflation factors should be below 5 or 10

```{r VIF_2}
car::vif(mod.PreMe)
```

**Impression:** Looks great, all VIFs are well below 5 (none are even above 2.5)

## Influential values

```{r}
model.data <- broom::augment(mod.PreMe) %>% 
  mutate(index = 1:n())

model.data %>%
  ggplot(aes(index, y=.cooksd)) + 
  geom_linerange(aes(ymin=0, ymax=.cooksd, color=PreMe), alpha=0.75) +
  geom_text(aes(x=index, y=.cooksd, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

model.data %>%
  ggplot(aes(index, .std.resid)) + 
  geom_point(aes(color = PreMe), alpha = .5) +
  geom_text(aes(x=index, y=.std.resid, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

plot(mod.PreMe, which = 5)
rm(model.data)
```


## Linearity

Check for a linear relationship between continuous predictors and logit of outcome

```{r linearity_2}
probabilities <- predict(mod.PreMe, type = "response")
BIRTH %>%
  select(-CASEID, -GA) %>%
  dplyr::select_if(is.numeric)  %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit) %>%
  
  ggplot(aes(logit, predictor.value))+
  geom_jitter(size = 0.5, alpha = 0.25) +
  geom_smooth(method = "glm", color="green") + 
  geom_smooth(method = "loess", color="red") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y") +
  labs(
    x = "Logit of predicted values",
    caption="Green line: generalised linear smooth\nRed line: Loess local smooth")
```

**Impression:** Well this looks better than the first regression. 

`Income` looks curvilinear, perhaps because the artificial grouping around 500% of the FPL and some effects for those in deep poverty (i.e. below 100%). `BMI` and `age` also have some problems at higher logit predicted values. Somewhat surprisingly, `YrEdu` doesn't look bad at all, and this is the only variable I'd really consider making categorical. 

## Prediction

Using adjusted rates (assuming other variables are at their means)

```{r predict2a}
### Predictions of propabilities ###
df <- tidyr::expand(BIRTH, Wanted, race) %>%
  mutate(
    GA = mean(BIRTH$GA),
    BMI = mean(BIRTH$BMI),
    age = mean(BIRTH$age),
    # income = mean(BIRTH$income),
    YrEdu  = mean(BIRTH$YrEdu)
  ) %>%
  expand_grid(
    income=seq(0, 500, by=50)
  ) 

# predict(object = mod.LBW, newdata = df, type="response", se.fit = T)
df$fit <- predict(object = mod.PreMe, newdata = df, type="response", se.fit = T)$fit
df$ul  <- df$fit + predict(object = mod.PreMe, newdata = df, type="response", se.fit = T)$se.fit
df$ll  <- df$fit - predict(object = mod.PreMe, newdata = df, type="response", se.fit = T)$se.fit

df %>% ggplot(aes(x=income, y=fit, group=race)) +
  geom_ribbon(aes(ymax=ul, ymin=ll), alpha=0.1) +
  geom_line(aes(color=race)) +
  facet_grid(.~Wanted) +
  scale_color_brewer(palette = "Dark2") +
  theme_bw() +
  labs(x="Income as % federal poverty line", y="Pr(Preterm Birth)",
       title="Predicted Pr(PTB), by income & race",
       subtitle = "Split by Wantedness (Yes/No)")
```







# Knew pregnant

Counts of who knew they were pregnant within the first 6 weeks of the pregnancy

```{r}
table(PN$KnowPreg) %>% 
  pander(caption="Counts of if mother knew pregnant or not within 6 weeks")
```

This model predicts the chance of subjects knowing they were pregnant within the first 6 weeks

```{r}
mod.KnowPreg <- glm(KnowPreg~PregNum + age + income + race + YrEdu + PregNum + Wanted, 
    data=PN, family=binomial)
pander(mod.KnowPreg)
```

Odds ratios

```{r}
# Tidy version of summary
broom::tidy(mod.KnowPreg) %>%
  select(term, estimate, p.value) %>%
  mutate(Odds.Ratio = exp(estimate)) %>%
  select(-estimate) %>%
  cbind(exp(broom::confint_tidy(mod.KnowPreg))) %>%
  mutate_if(is.numeric, round, 4) %>%
  filter(term!="(Intercept)") %>%
  select(term, Odds.Ratio, conf.low, conf.high, p.value) %>%
  # mutate(Odds.Ratio = round(Odds.Ratio, 3)) %>%
  # mutate(conf.low   = round(conf.low, 3)) %>%
  # mutate(conf.high  = round(conf.high, 3)) %>%
  # pander()
  mutate(
    p.value    = cell_spec(p.value, color = ifelse(p.value <0.05, "red", "black"))
    ) %>%
  knitr::kable(digits=4, escape=F) %>%
  kable_styling("striped", full_width = F)
```


```{r}
glance(mod.KnowPreg) %>% pander()
```

```{r OR_plot3, include=F}
sjPlot::plot_model(mod.KnowPreg, title = "OR of knowing pregnancy by 6 wk",
                   sort.est=T, show.values = T, value.offset = .4)
```


## Type III Anova

```{r ANOVA_3}
# type 3 anova
car::Anova(mod.KnowPreg, type = 3)
```

## Hosmer-Lemeshow & more

```{r diagnostics_3}
# diagnostics
rms::lrm(mod.KnowPreg[["formula"]], 
         data=PN) %>% 
   pander(coefs=F)

# Hosmer-Lemeshow Goodness of Fit (GOF) Test
ResourceSelection::hoslem.test(
  ifelse(PN$KnowPreg=="Yes", 1, 0),
  fitted(mod.KnowPreg))
```

**Impression:** Everything looks good here

## Multicolinearity / VIF

Variance inflation factors should be below 5 or 10

```{r VIF_3}
car::vif(mod.KnowPreg)
```

**Impression:** Looks great, all VIFs are well below 5 (none are even above 2.5)

## Influential values

```{r}
model.data <- broom::augment(mod.KnowPreg) %>% 
  mutate(index = 1:n())

model.data %>%
  ggplot(aes(index, y=.cooksd)) + 
  geom_linerange(aes(ymin=0, ymax=.cooksd, color=KnowPreg), alpha=0.75) +
  geom_text(aes(x=index, y=.cooksd, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

model.data %>%
  ggplot(aes(index, .std.resid)) + 
  geom_point(aes(color = KnowPreg), alpha = .5) +
  geom_text(aes(x=index, y=.std.resid, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

plot(mod.KnowPreg, which = 5)
rm(model.data)
```

## Linearity

Check for a linear relationship between continuous predictors and logit of outcome

```{r linearity_3}
probabilities <- predict(mod.KnowPreg, type = "response")
PN %>%
  select(-CASEID, -GA) %>%
  dplyr::select_if(is.numeric)  %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit) %>%
  
  ggplot(aes(logit, predictor.value))+
  geom_jitter(size = 0.5, alpha = 0.25) +
  geom_smooth(method = "glm", color="green") + 
  geom_smooth(method = "loess", color="red") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y") +
  labs(
    x = "Logit of predicted values",
    caption="Green line: generalised linear smooth\nRed line: Loess local smooth")
```

**Impression:** Unlike the last two regressions, `age` and `BMI` look pretty linear. Again `income` appears to deviate from linearity once it's below 100% of the FPL, and it looks like there might be a difference in `YrEdu` for those with less than a high school education (YrEdu < 12) and everyone else. Finally, `PregNum` looks to be linear despite the few outliers (who were having their 10th or greater pregnancy).

## Prediction

```{r Prediction_3a}
### Predictions of propabilities ###
df <- tidyr::expand(PN, Wanted, race) %>%
  mutate(
    PregNum = mean(PN$PregNum),
    age = mean(PN$age),
    # income = mean(PN$income),
    # race = as.factor("White"),
    YrEdu  = mean(PN$YrEdu)
  ) %>%
  expand_grid(
    income=seq(0, 500, by=50)
  ) 


df$fit <- predict(object = mod.KnowPreg, newdata = df, type="response", se.fit = T)$fit
df$ul  <- df$fit + predict(object = mod.KnowPreg, newdata = df, type="response", se.fit = T)$se.fit
df$ll  <- df$fit - predict(object = mod.KnowPreg, newdata = df, type="response", se.fit = T)$se.fit

df %>% 
  ggplot(aes(x=income, y=fit, group=Wanted)) +
  geom_ribbon(aes(ymax=ul, ymin=ll), alpha=0.1) +
  geom_line(aes(color=Wanted, linetype=Wanted)) +
  facet_grid(.~race) +
  scale_color_colorblind() +
  theme_bw() +
  labs(x="Income as % federal poverty line", y="Pr(Knew Pregnant)",
       title="Predicted Pr(KnowPreg)",
       subtitle = "Split by race")
```



# Got prenatal care

Counts of who got prenatal care in the first trimester

```{r}
table(PN$gotPNcare) %>% 
  pander(caption="Counts of if mother got prenatal care in the first trimester")
```

```{r}
mod.gotPNcare <- glm(gotPNcare~ KnowPreg + age + income + race + YrEdu + PregNum + Wanted, 
    data=PN, family=binomial)
pander(mod.gotPNcare)
```

Odds ratios

```{r}
# Tidy version of summary
broom::tidy(mod.gotPNcare) %>%
  select(term, estimate, p.value) %>%
  mutate(Odds.Ratio = exp(estimate)) %>%
  select(-estimate) %>%
  cbind(exp(broom::confint_tidy(mod.gotPNcare))) %>%
  mutate_if(is.numeric, round, 4) %>%
  filter(term!="(Intercept)") %>%
  select(term, Odds.Ratio, conf.low, conf.high, p.value) %>%
  # mutate(Odds.Ratio = round(Odds.Ratio, 3)) %>%
  # mutate(conf.low   = round(conf.low, 3)) %>%
  # mutate(conf.high  = round(conf.high, 3)) %>%
  # pander()
  mutate(
    p.value    = cell_spec(p.value, color = ifelse(p.value <0.05, "red", "black"))
    ) %>%
  knitr::kable(digits=4, escape=F) %>%
  kable_styling("striped", full_width = F)
```


```{r}
glance(mod.gotPNcare) %>% pander()
```


```{r OR_plot4, include=F}
sjPlot::plot_model(mod.gotPNcare, title = "OR of getting prenatal care",
                   sort.est=T, show.values = T, value.offset = .4)
```


## Type III Anova

```{r ANOVA_4}
# type 3 anova
car::Anova(mod.gotPNcare, type = 3)
```

## Hosmer-Lemeshow & more

```{r diagnostics_4}
# diagnostics
rms::lrm(mod.gotPNcare[["formula"]], 
         data=PN) %>% 
   pander(coefs=F)

# Hosmer-Lemeshow Goodness of Fit (GOF) Test
ResourceSelection::hoslem.test(
  ifelse(PN$gotPNcare=="Yes", 1, 0),
  fitted(mod.gotPNcare))
```

**Impression:** Everything looks good here

## Multicolinearity / VIF

Variance inflation factors should be below 5 or 10

```{r VIF_4}
car::vif(mod.gotPNcare)
```

**Impression:** Looks great, all VIFs are well below 5 (none are even above 2.5)

## Influential values

```{r}
model.data <- broom::augment(mod.gotPNcare) %>% 
  mutate(index = 1:n())

model.data %>%
  ggplot(aes(index, y=.cooksd)) + 
  geom_linerange(aes(ymin=0, ymax=.cooksd, color=gotPNcare), alpha=0.75) +
  geom_text(aes(x=index, y=.cooksd, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

model.data %>%
  ggplot(aes(index, .std.resid)) + 
  geom_point(aes(color = gotPNcare), alpha = .5) +
  geom_text(aes(x=index, y=.std.resid, label=index),
                           data=top_n(model.data, n=3, wt=.cooksd)) +
  theme_bw()

plot(mod.gotPNcare, which = 5)
rm(model.data)
```


## Linearity

Check for a linear relationship between continuous predictors and logit of outcome

```{r linearity_4}
probabilities <- predict(mod.gotPNcare, type = "response")
PN %>%
  select(-CASEID, -GA) %>%
  dplyr::select_if(is.numeric)  %>%
  mutate(index = 1:n()) %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  mutate(KnowPreg = PN$KnowPreg) %>%
  gather(key = "predictors", value = "predictor.value", -logit, -index, -KnowPreg) %>%
  filter(index!=404) %>%
  filter(index!=402) %>% # remove two pregnancies
  
  ggplot(aes(logit, predictor.value))+
  geom_jitter(size = 0.5, alpha = 0.25, aes(color=KnowPreg)) +
  geom_smooth(method = "glm", color="green", aes(group=KnowPreg)) + 
  geom_smooth(method = "loess", color="red") + 
  theme_bw() + theme(legend.position="bottom") +
  facet_wrap(~predictors, scales = "free_y") +
  labs(
    x = "Logit of predicted values",
    caption="Green line: generalised linear smooth\nRed line: Loess local smooth")
```

**Impression:** Because the x-axis is a function of the predicted values and we included `KnowPreg` in our regression (with a massive effect size), these plots spread into two distict groups based on this predictor. The overall loess line (red) clearly doesn't demonstrate a linear relationship, but within each group it looks like there's better linearity. At this point I'm not sure what my impression should be, so I'll just leave it there and let you tell me the answer. Is this too problematic to accept regression 4 as valid? 

_Note: Two pregnancies (with pregnum=14) were removed_

## Prediction

```{r Prediction_4a}
### Predictions of propabilities ###
df <- tidyr::expand(PN, Wanted, KnowPreg) %>%
  mutate(
    age = mean(PN$age),
    # PregNum = mean(PN$PregNum),
    # income = mean(PN$income),
    race = as.factor("White"),
    YrEdu  = mean(PN$YrEdu)
  ) %>%
  expand_grid(
    income=seq(0, 500, by=50)
  ) %>%
  expand_grid(
    PregNum = seq(1, 7, by=2)
  )


df$fit <- predict(object = mod.gotPNcare, newdata = df, type="response", se.fit = T)$fit
df$ul  <- df$fit + predict(object = mod.gotPNcare, newdata = df, type="response", se.fit = T)$se.fit
df$ll  <- df$fit - predict(object = mod.gotPNcare, newdata = df, type="response", se.fit = T)$se.fit

df %>% 
  mutate(PregNum = paste0("Pregnancy #", PregNum)) %>%
  ggplot(aes(x=income, y=fit, group=interaction(Wanted, KnowPreg))) +
  geom_ribbon(aes(ymax=ul, ymin=ll), alpha=0.1) +
  geom_line(aes(color=Wanted, linetype=KnowPreg)) +
  facet_grid(.~PregNum) +
  scale_color_colorblind() +
  theme_bw() +
  labs(x="Income as % federal poverty line", y="Pr(Receiving Prenatal Care)",
       title="Predicted Pr(gotPNcare)",
       subtitle = "Split by pregnancy number")
```


# A bit on linearity

I've read mixed information on what is important for the assumption of linearity, and in the plots above I've look at linear relationships between predictors and the _logit of the predicted outcome_. After the presentation in class (and more reading) I'm adding this section looking at the linear relationships between the predictors and their _pearson residuals_ using the `car` package.

From what I'm able to gather from [reading up on the issue](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4885900/), the _pearson residuals_ vs _predictor_ line should be straight and have a slope of zero. Additionally, the `car::residualPlots()` function provides a lack of fit test for each variables' relationship with the residuals.

## Low birth weight

```{r resLinearity_1, fig.height = 7}
car::residualPlots(mod.LBW)
```

## Premature birth

```{r resLinearity_2, fig.height = 7}
car::residualPlots(mod.PreMe)
```

## Knew pregnant

```{r resLinearity_3, fig.height = 7}
car::residualPlots(mod.KnowPreg)
```

## Got prenatal care

```{r resLinearity_4, fig.height = 7}
car::residualPlots(mod.gotPNcare)
```